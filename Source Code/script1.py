# -*- coding: utf-8 -*-
"""assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WSE84TW6VzTT9RsyXQtjotO0hIxCMvkC
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Change directory to shared folder
import os
shared_folder = "/content/drive/MyDrive/DSCI 550: Team Folder/Assignment1_files"
os.chdir(shared_folder)

# List files in the shared folder
os.listdir()

"""# Loading Raw Dataset File"""

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
import re

# Download the necessary resources
nltk.download('punkt')
nltk.download('punkt_tab')

# Load the CSV file
df = pd.read_csv('haunted_places.csv')
print('DataFrame created from : haunted_places.csv')

"""# Data Cleaning
The haunted_places.csv contains unstructured description text.
Cleaning strings of tokenized words (removing punctuations and capitalizations)
"""

def clean_text(text):
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove all punctuation and non-alphanumeric characters, keep only words
    tokens = [word for word in tokens if word.isalnum()]
    # Join tokens into a single space-separated string
    cleaned_text = ' '.join(tokens)
    return cleaned_text

# Apply the function to each cell in the DataFrame
df['clean_description'] = df['description'].apply(clean_text)

# Download results
df.to_csv('cleaned_haunted_file.csv', index=False)

df.clean_description.head()

def clean_misspellings(df, column, corrections: dict):
    for misspelling, correct in corrections.items():
        df[column] = df[column].str.replace(fr'\b{misspelling}\b', correct, regex=True)
        df[column] = df[column].str.replace(fr'\b{misspelling}s\b', correct + 's', regex=True)
    df['clean_spelling'] = df[column]
    return df

# Dictionary of misspellings - {Misspelled word: Correct word}, simple 's' pluralization is accounted for.
corrections = {'sprit': 'spirit',
               'gost': 'ghost',
               'aparition': 'apparition', 'apperition': 'apparition',
               'poltergiest': 'poltergeist',
               'figer': 'figure'}

# Apply function
df = clean_misspellings(df, 'clean_description', corrections)
print("df['clean_spelling'] created")
# Download results
#df.to_csv('cleaned_spelling_haunted_file.csv', index=False)
#print('Downloaded cleaned_spelling_haunted_file.csv')

"""Function to search specified misspelled words and replace them with the correct word.

# Featurization (LLM)

## LLM (Audio Evidence)
"""

import openai
import pandas as pd
from tqdm import tqdm
import re

# initialize OpenAI client
client = openai.OpenAI(api_key="Key-HERE")

df_raw = pd.read_csv('haunted_places.csv')


# select a random sample
# df_sample = df.sample(n=20, random_state=25).copy()

def extract_audio_evidence(description):
    prompt = f"""
    You are analyzing descriptions of haunted locations to determine if they contain **audio evidence** of paranormal activity.

    - Only consider sounds that are **explicitly linked to a supernatural presence**, such as **unexplained whispers, ghostly voices, eerie sounds, disembodied footsteps, supernatural noises, or loud bangs with no source**.
    - Ignore background noises like **wind, animals, creaking houses, traffic, or other natural/environmental sounds** unless connected to the paranormal event.

    ### **Task**
    Analyze the following description and determine **if it contains audio evidence of paranormal activity**. Your answer must be in the format:

    **Answer:** Yes or No
    **Reasoning:** [Brief explanation of why audio evidence is or is not present]

    ### **Example 1 (Audio Evidence Present)**
    **Description:**
    _"Visitors often hear whispers echoing through the empty halls at night. Some claim to hear their names being called when no one is there."_

    **Expected Response:**
    **Answer:** Yes
    **Reasoning:** The description explicitly mentions unexplained whispers and voices, which are directly tied to paranormal activity.

    ---
    ### **Example 2 (No Audio Evidence)**
    **Description:**
    _"A woman in a white dress has been spotted near the graveyard, standing motionless under the moonlight."_

    **Expected Response:**
    **Answer:** No
    **Reasoning:** There is no mention of sound-related experiences in this description.
    ---
    ### **Now analyze this description:**
    {description}
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100,
            temperature=0.1
        )

        # obtain response(this was for testing)
        content = response.choices[0].message.content.strip()
        # print("RAW RESPONSE:", content)

        # get yes/no answer
        answer_match = re.search(r"\*\*Answer:\*\*\s*(Yes|No)", content, re.IGNORECASE)
        reasoning_match = re.search(r"\*\*Reasoning:\*\*\s*(.*)", content, re.DOTALL)

        answer = answer_match.group(1) if answer_match else "Not Found"
        reasoning = reasoning_match.group(1).strip() if reasoning_match else "Not Found"

        # binary answer
        return pd.Series([answer, reasoning])

    except Exception as e:
        return pd.Series([f"Error: {str(e)}", ""])

# apply to 2 columns
tqdm.pandas()
df[["Audio Evidence", "Reasoning"]] = df["description"].progress_apply(extract_audio_evidence)

# save
df.to_csv("haunted_places_sample_audio.csv", index=False)

"""Read the csv input (haunted_places_sample_audio.csv) of the Audio Evidence LLM output.
Then append those columns to the main df
"""

df_audio = pd.read_csv("haunted_places_sample_audio.csv", sep=",")
df_audio.head()

df = df.join(df_audio[['Audio Evidence', 'Reasoning']])
df.head()

df.rename(columns={'Reasoning': 'Audio Reasoning'}, inplace=True)
print(df.columns)

print(df.columns)

"""## Time of Day

**Time of day evidence code**
"""

##uncomment this to install the keras tensorflow
## pip install tensorflow
# pip install tf-keras

## This feature approach return the time of the day label using zero-shot classification
## of the zero=shot classification from facebool/bart-large-mnli


import pandas as pd
from transformers import pipeline

# Initialize the NLI pipeline using a pre-trained model from Hugging Face
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Function to classify time of day using NLI
def classify_time_of_day(text):
    # Define the possible labels (morning, evening, dusk, unknown)
    candidate_labels = ["Morning", "Evening", "Dusk", "Unknown"]

    # Use the zero-shot classification to classify the text
    result = classifier(text, candidate_labels)

    # Return the label with the highest score
    return result['labels'][0]

# Function to process the CSV and add the time of day column
def update_time_of_day(input_file):
    # Read the CSV file into a pandas DataFrame
    df_time_of_day = pd.read_csv(input_file)

    # Check if 'description' column exists
    if 'description' not in df.columns:
        print("Error: The CSV file must contain a 'description' column with text data.")
        return

    # Create a list to hold the time of day results
    time_of_day_list = []

    # Iterate through each row in the DataFrame
    for index, row in df.iterrows():
        text = row['description']  # Get the text data from the 'description' column

        # Use the NLI classifier to determine the time of day
        time_of_day = classify_time_of_day(text)
        time_of_day_list.append(time_of_day)

    # Add the 'time_of_day' column to the DataFrame
    df_time_of_day['time_of_day'] = time_of_day_list

    # Save the updated DataFrame to a new CSV file
    df.to_csv('time_of_day_file.csv', index=False)
    print("Updated file saved as 'time_of_day_file.csv'.")

# Example usage: Pass your CSV file path as an argument
update_time_of_day('cleaned_haunted_file.csv')

df_time_of_day = pd.read_csv("updated_time_of_day_file.csv", sep=",")
df_time_of_day.head()

df = df.join(df_time_of_day[['time_of_day']])
df.head()

"""## **Image/Video/Visual Evidence**

"""

import openai
import pandas as pd
from tqdm import tqdm
import os
import re


# initialize OpenAI client
client = openai.OpenAI(api_key="API KEY")

# Normalize column names (remove spaces, convert to lowercase)
df.columns = df.columns.str.lower().str.strip()

# Check if "description" column exists
if "description" not in df.columns:
    raise KeyError("The column 'description' is missing from the dataset. Check your CSV file.")

# Define function to extract visual evidence
def extract_visual_evidence(description):
    prompt = f"""
    Does the following text specifically mention existing images, videos, or photographic evidence of paranormal activity?
    Only count it as 'Yes' if the text explicitly states that a photo, video, or recorded footage exists.
    Include security camera footage only if it captured something unusual, but ignore mentions of security cameras in general or warnings about surveillance if it does not have to do with paranormal activity.
    Ignore any mentions of security guards recording trespassers, people seeing something with their own eyes, or unverified claims.
    Count news reports if they state a photo or video was included.
    Include historical or archival footage if it is still available.
    Include multiple witness reports if they claim to have recorded evidence.

    **Task:**
    Analyze the following description and determine if it contains visual evidence of paranormal activity.
    Your answer must be in the following format:

    **Answer:** Yes or No
    **Reasoning:** [Brief explanation of why visual evidence is or is not present]

    **Example 1 (Visual Evidence Present)**
    **Description:**
    _"Denton Rd. - There's an overall bad feeling in this cemetery. If you take a camera (preferably digital) and take pictures, you'll see orbs everywhere. There's a specific large, greenish orb that shows up in photographs quite a bit."_

    **Expected Response:**
    **Answer:** Yes
    **Reasoning:** The description explicitly mentions unexplained green orbs that are in photographs, which are directly tied to paranormal activity.

    **Example 2 (No Visual Evidence)**
    **Description:**
    _"People report hearing footsteps, and doors slamming where there are no doors, a small child's voice say 'Mama?'."_

    **Expected Response:**
    **Answer:** No
    **Reasoning:** There is no mention of visual-related experiences in this description.

    ---
    ### **Now analyze this description:**
    {description}
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100,
            temperature=0.1
        )

        # Obtain raw response
        content = response.choices[0].message.content.strip()
        # print("RAW RESPONSE:", content)  # Debugging

        # Improved regex parsing
        answer_match = re.search(r"(?i)\*\*Answer:\*\*\s*(True|False)", content)
        reasoning_match = re.search(r"(?i)\*\*Reasoning:\*\*\s*(.+)", content, re.DOTALL)

        answer = answer_match.group(1) if answer_match else "Unknown"
        reasoning = reasoning_match.group(1).strip() if reasoning_match else "Parsing error"

        return pd.Series([answer, reasoning])

    except Exception as e:
        return pd.Series([f"Error: {str(e)}", ""])

# Apply function to DataFrame with tqdm progress bar
tqdm.pandas()
df[["visual_evidence", "reasoning"]] = df["description"].progress_apply(extract_visual_evidence)

# Save results
df.to_csv("haunted_places_visual_evidence.csv", index=False)
print("Processing complete. File saved as 'cleaned_haunted_file.csv'.")

df_visual_evidence = pd.read_csv("haunted_places_visual_evidence.csv", sep=",")
df_visual_evidence.head()

df = df.join(df_visual_evidence[["visual_evidence", "reasoning"]])
df.head()

df.rename(columns={'visual_evidence': 'Visual Evidence', 'reasoning': 'Visual Reasoning'}, inplace=True)

df.head()

"""## Witness Count
Witness count parser + LLM extraction = ~30% 0's (As long as the scene was witness/seen by anyone, it should be counted; prior would exclude 1st person; included example as references, pattern box with REGEX)

Hybrid approach: Pre-process with library/REGEX -->
- Multiple and nuanced approach to testing/trial -> long delay in running operations
"""

import openai
import pandas as pd
from tqdm import tqdm
import re
from number_parser import parse

# initialize OpenAI client
client = openai.OpenAI(api_key="KEY-HERE")

# read in df
df_witness = pd.read_csv("df_with_img_feat.csv", sep=",")

# select a random sample for testing
# df_sample = df.sample(n=50, random_state=24).copy()


def extract_candidate_number(description):
    """
    uses regex patterns along with number-parser to extract a candidate  for the witness count.
    looks for the following phrases
    """
    patterns = [
        r"I saw ([A-Za-z0-9\-]+)",
        r"I experienced ([A-Za-z0-9\-]+)",
        r"I witnessed ([A-Za-z0-9\-]+)",
        r"(\w+)\s+visitors",
        r"(\w+)\s+onlookers",
        r"(\w+)\s+people",
        r"(\w+)\s+individuals",
        r"(\w+)\s+witnesses",
        r"(\w+)\s+bystanders",
        r"(\w+)\s+spectators",
        r"at least (\w+)",
        r"no less than (\w+)",
        r"around (\w+)",
        r"approximately (\w+)",
        r"witnessed by (\w+)"

    ]
    for pattern in patterns:
        match = re.search(pattern, description, re.IGNORECASE)
        if match:
            candidate_str = match.group(1)
            try:
                # Try direct integer conversion first.
                candidate_num = int(candidate_str)
            except ValueError:
                # If that fails, use number-parser to convert textual numbers.
                candidate_num = parse(candidate_str)
            if candidate_num is not None:
                return str(candidate_num)
    return "0"


def refine_witness_count(description, candidate):
    """
    uses the LLM to evaluate whether the candidate witness count is correct.
    if not, the model should output the corrected witness count and justification.
    """
    prompt = f"""
    You are an expert in sentiment analysis and contextual reading. We have automatically extracted a candidate witness count of "{candidate}" from the following description:

    {description}

    Based on the context, tone, and details provided, please evaluate whether this candidate accurately represents the number of witnesses to the paranormal event. If it is correct, confirm it and explain your reasoning. If it is not correct, please provide the corrected witness count and justify your answer.

    Please consider:
    - If the narrator explicitly states their own experience (e.g., "I saw", "I experienced", or "I witnessed") and no other witnesses are mentioned, count the narrator as one witness.
    - If the description clearly states or implies that multiple people witnessed the event, use that number.
    - Ignore numbers that refer to unrelated details (e.g., "5 bodies" or "3 floors").
    - If the description only provides a lower bound or a tentative figure (for example, using qualitative clues such as "some", "a couple", etc.), output the floor value and append a '+' sign (e.g., "2+") to indicate it may be higher.
    - Default to 0 if, after a thorough analysis, there is an indication that nobody witnessed the event. Use fair judgement, but do not go overboard stretching the text to find a non-existent witnesses.
    - If the event was witnessed or seen/experienced by someone, even if not specified by exactly who, they should count as a witness. ex.
      The Richardi House (now known as The Grand Victorian Bed & Breakfast) is rumored to be haunted by Henri Richardi. Richardi built the house for his intended bride. When she left him for another man, he was heartbroken. Strange lights were seen in the cupola when the house was uninhabited (before being turned into a Bed & Breakfast). This should yield at least one person as it claims the lights were "seen" even if we dont know by who.
    - Only provide numbers for count, do not give answers like. "many" or "a lot"

    Format your answer exactly as follows:

    **Correct Witness Count:** [number or number with a '+' if it's a floor estimate]
    **Reasoning:** [A brief explanation of your evaluation and how you arrived at the number]
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100,
            temperature=0.1
        )
        content = response.choices[0].message.content.strip()
        # debugging output
        print("Refinement RAW RESPONSE:", content)

        # extract the witness count and reasoning from the LLM's response.
        witness_match = re.search(r"\*\*Correct Witness Count:\*\*\s*([A-Za-z0-9\s\+\-]+)", content, re.IGNORECASE)
        reasoning_match = re.search(r"\*\*Reasoning:\*\*\s*(.*)", content, re.DOTALL)
        witness_count_raw = witness_match.group(1).strip() if witness_match else candidate
        reasoning = reasoning_match.group(1).strip() if reasoning_match else "Not Found"

        # process the extracted witness count:
        if "plus" in witness_count_raw.lower() or '+' in witness_count_raw:
            number_match = re.search(r"(\d+)", witness_count_raw)
            if number_match:
                witness_count = number_match.group(1) + "+"
            else:
                witness_count = witness_count_raw
        else:
            try:
                witness_count = str(int(witness_count_raw))
            except ValueError:
                witness_count = "0"

        return pd.Series([witness_count, reasoning])
    except Exception as e:
        return pd.Series([f"Error: {str(e)}", ""])



def process_witness_count(description):
    candidate = extract_candidate_number(description)
    return refine_witness_count(description, candidate)


# apply function to sample
tqdm.pandas()
df[["Witness Count", "Witness Reasoning"]] = df_witness["description"].progress_apply(process_witness_count)

# save
df.to_csv("haunted_places_with_witness_Final.csv", index=False)

df_witness = pd.read_csv("haunted_places_with_witness_Final.csv", sep=",")
df_witness.head()

df = df.join(df_witness[["Witness Count", "Witness Reasoning"]])
df.head()

"""# Featurization (Keyword Matching & Word Vectorization)

## Helper Tools
Helper tool to look for a specific keyword within a dataframe column.
Capitalization doesn't matter. Spelling and characters do matter.
"""

def search_words_in_column(df, column, words: list):
    results = []
    total_counts = {word: 0 for word in words}
    rows_found = {word: 0 for word in words}

    for index, row in df.iterrows():
        text = row[column].lower()

        row_counts = {word: text.split().count(word.lower()) for word in words}  # Count each word

        for word, count in row_counts.items():
            if count > 0:
                results.append({'Row Index': index, 'Word': word, 'Count': count, 'Text': row[column]})
                total_counts[word] += count
                rows_found[word] += 1

    for word in words:
        if total_counts[word] > 0:
          print(f"The word '{word}' was found {total_counts[word]} times in {rows_found[word]} rows.")
        else:
          continue

    return results

potential_misspelled_words = [
    "gost", "ghots", "gohst", "ghst",  # ghost
    "sprit", "spirrit", "spiret", "spirut",  # spirit
    "appirition", "apparitian", "appariton", "aparition", "apperition",  # apparition
    "shaddow", "shado", "schadow", "shawdow",  # shadow
    "fantom", "phantum", "phantam", "phentom",  # phantom
    "poltergiest", "poltergeest", "poltergyst", "poltergiast",  # poltergeist
    "deamon", "deman", "daimon", "demonn",  # demon
    "spectre", "spectar", "spectir", "spekter",  # specter
    "orbe", "orbb", "obre",  # orb
    "figer", "figyre", "figgur", "figuire",  # figure
    "entiti", "entety", "entitty", "entety",  # entity
    "miste", "misst", "mistt",  # mist
    "presense", "presance", "pressence", "prescence",  # presence
    "wich", "whitch", "wytch", "witsh",  # witch
    "vamire", "vampyre", "vampeer", "vampir",  # vampire
    "warlok", "warloc", "worlock", "wurlock",  # warlock
    "werewulf", "wearwolf", "werwolf", "werwolv",  # werewolf
    "trol", "trole", "trall", "trowl",  # troll
    "impa", "ipm", "impy", "impz"  # imp
]

print(search_words_in_column(df, 'description', potential_misspelled_words))

def download_csv(df, file_output_name):
  df.to_csv(file_output_name, index=False)
  print(f'Downloaded {file_output_name}')

#Test call
#download_csv(df, 'test2_haunted_file.csv')

"""## Tokenization

This takes the cleaned descriptions and splits up the description into tokenization to prepare it for single keyword matching approach.
"""

def tokenize_and_filter(text):
    # Tokenize
    tokens = word_tokenize(text)
    # Remove any token that contains a digit
    tokens = [t for t in tokens if t.isalpha()]
    return tokens

df['tokenized'] = df['clean_spelling'].apply(tokenize_and_filter)

"""## Part of Speech Tagging
This model looks at the tokenized description and predicts what part of speech it likely is. The intent is to be able to later extract adjectives that describe apparition_types and to get the syntactic function of tokenized words in relation to the sentence.
"""

from nltk import pos_tag
nltk.download('averaged_perceptron_tagger_eng')

df['pos_tokenized'] = df['tokenized'].apply(pos_tag)
print(df['pos_tokenized'])

def check_pos_tags_for_row(df, row_index):
    try:
        # Get the tokenized and POS-tagged tokens for the specific row
        pos_tags = df.iloc[row_index-2]['pos_tokenized']
        # Create a single line string of the word-POS tag pairs
        pos_tags_line = ', '.join([f"{word}: {tag}" for word, tag in pos_tags])
        print(f"POS Tags for Row {row_index}: {pos_tags_line}")
    except KeyError:
        print("The column 'tokenized_pos' does not exist.")
    except IndexError:
        print(f"Row index {row_index} is out of range.")

# Example usage:
check_pos_tags_for_row(df, 354)

#Proper Noun removal. Needed later for Word Embedding
from nltk import pos_tag
nltk.download('averaged_perceptron_tagger_eng')

def remove_proper_nouns(tokens):
    tagged = pos_tag(tokens)  # [('Josh', 'NNP'), ('spirit', 'NN'), ...]
    filtered = [word.lower() for word, tag in tagged if tag not in ['NNP', 'NNPS']]
    return filtered

df['filtered_tokenized'] = df['tokenized'].apply(remove_proper_nouns)

"""## Lemmatization"""

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(token) for token in tokens]

df['lemma_filtered_tokenized'] = df['filtered_tokenized'].apply(lemmatize_tokens)

# Download results
#df.to_csv('token_lemma_haunted_places.csv', index=False)
#print('Downloaded token_lemma_haunted_places.csv')

df.filter(items=["clean_spelling", "tokenized", "filtered_tokenized", "lemma_filtered_tokenized"]).head()

from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ  # Adjective
    elif tag.startswith('V'):
        return wordnet.VERB  # Verb
    elif tag.startswith('N'):
        return wordnet.NOUN  # Noun
    elif tag.startswith('R'):
        return wordnet.ADV  # Adverb
    else:
        return wordnet.NOUN  # Default to noun


lemmatizer = WordNetLemmatizer()

def lemmatize_tokens_pos(tokens_pos):
    return [lemmatizer.lemmatize(word.lower(), get_wordnet_pos(tag)) for word, tag in tokens_pos]

df['lemma_pos_tokenized'] = df['pos_tokenized'].apply(lemmatize_tokens_pos)

df.filter(items=["clean_spelling", "tokenized", "pos_tokenized", "lemma_pos_tokenized"]).head()

#Download results
# df.to_csv('token_lemma_haunted_places.csv', index=False)
# print('Downloaded token_lemma_haunted_places.csv')

#Expected to have a main df with the cleaned columns, LLM features, and token/POS/Lemma
print(df.columns)

"""## Match and Extract Keyword"""

def extract_keywords(text, keywords):
    """Extract keywords from the text."""
    text = text.lower()  # Convert to lowercase
    keywords = [str(word) for word in keywords if pd.notna(word)]
    matches = [word for word in keywords if re.search(r'\b' + word + r'(s)?\b', text)]
    return matches

"""## Keyword Bank File Download"""

#Reading the Keyword bank
keyword_bank_df = pd.read_csv('keywords_dictionary.csv', encoding = 'latin1')

"""## Event Type

### Get Data
1. Haunted Places Dataframe ← haunted_df
2. Keywords Dataframe and List of Event Types ← kw_df, events
"""

import pandas as pd
import re

events = ["Suicide", "Murder", "Natural Disaster", "Death", "Supernatural"]
target_cols = [f"Event: {x}" for x in events]

haunted_df = df.copy()
kw_df = keyword_bank_df[target_cols]
kw_df = kw_df.rename(columns=lambda x: x.replace('Event: ', ''))

kw_df.head()

haunted_df.head()

"""### Get Event Column
Methods defined:
1. Get event from description ← get_event()
2. Get event column from DataFrame ← get_event_col()
"""

def get_event(description, keywords_df, events) -> str:
  description = description.lower()
  for event in events:
    for keyword in keywords_df[event].dropna():
      regex = re.compile(keyword)
      if regex.search(description):
        return event
  return "Unknown"

def get_event_col(df, keywords_df, events) -> list[str]:
  event_col = []

  for _, row in df.iterrows():
    description = row['clean_spelling']
    event = get_event(description, keywords_df, events)
    event_col.append(event)

  return event_col

haunted_df["Event"] = get_event_col(haunted_df, kw_df, events)

df["Event"] = get_event_col(df, kw_df, events)

"""### Results and Analysis
1. Result: haunted_df['Event']
2. Counts for each event type
3. Random sample for given event type ← get_sample()
"""

print("Event Counts")
print("")
for event in events + ['Unknown']:
  print(f"{event}: {haunted_df[haunted_df['Event'] == event].shape[0]}")

unknown_count = haunted_df[haunted_df['Event'] == 'Unknown'].shape[0]  # Count 'unknown' (case insensitive)
total_count = haunted_df.shape[0]  # Total number of rows
print(f"Unknown count: {unknown_count}, Total row count: {total_count}, Unknown %: {round(unknown_count/total_count*100,1)}%, Classified %:{100-round(unknown_count/total_count*100,1)}%")

def get_sample(df, event) -> pd.DataFrame:
  return df[df['Event'] == event].sample(1).description.values[0]

get_sample(haunted_df, "Murder")

"""## Apparition Types

### Extract keywords
"""

#Grabs the keyword columns from the keywords dictionary and extracts those keywords from the processed dataframe
df['apparition_types'] = df['lemma_pos_tokenized'].apply(lambda x: extract_keywords(str(x), keyword_bank_df['Apparition_Types']))
df['apparition_descriptors'] = df['lemma_pos_tokenized'].apply(lambda x: extract_keywords(str(x), keyword_bank_df['Apparition_Descriptors']))
df['apparition_gender'] = df['tokenized'].apply(lambda x: extract_keywords(str(x), keyword_bank_df['Apparition_Gender']))
df['apparition_age'] = df['tokenized'].apply(lambda x: extract_keywords(str(x), keyword_bank_df['Apparition_Age']))

print(df[['apparition_types', 'apparition_descriptors', 'apparition_gender', 'apparition_age']].head())

#Turn empty lists into [Unknown]
df['apparition_types'] = df['apparition_types'].apply(lambda x: ['Unknown'] if isinstance(x, list) and not x else x)
df['apparition_descriptors'] = df['apparition_descriptors'].apply(lambda x: ['Unknown'] if isinstance(x, list) and not x else x)
df['apparition_gender'] = df['apparition_gender'].apply(lambda x: ['Unknown'] if isinstance(x, list) and not x else x)
df['apparition_age'] = df['apparition_age'].apply(lambda x: ['Unknown'] if isinstance(x, list) and not x else x)

print(df[['apparition_types', 'apparition_descriptors', 'apparition_gender', 'apparition_age']].head())

#Re-format the output of a list into a separate column as a string
df['apparition_types_str'] = df['apparition_types'].apply(' '.join)
df['apparition_descriptors_str'] = df['apparition_descriptors'].apply(' '.join)
df['apparition_gender_str'] = df['apparition_gender'].apply(' '.join)
df['apparition_age_str'] = df['apparition_age'].apply(' '.join)

print(df[['apparition_types_str', 'apparition_descriptors_str', 'apparition_gender_str', 'apparition_age_str']].head())

df_apparition = df[['city', 'country','description', 'location', 'state', 'state_abbrev', 'longitude', 'latitude','city_longitude', 'city_longitude', 'tokenized', 'pos_tokenized','lemma_pos_tokenized', 'apparition_types', 'apparition_descriptors', 'apparition_gender', 'apparition_age', 'apparition_types_str', 'apparition_descriptors_str', 'apparition_gender_str', 'apparition_age_str']]
df_apparition.to_csv('haunted_file_apparition_extract.csv', index=False)
print(f"CSV file saved 'haunted_file_apparition_extract'")

"""### Initial Summary Counts"""

from collections import Counter
# Flatten the list for easier counting of appartition_types

def count_word_frequncy(*df_columns):
    counts_dict = {}

    for df_column in df_columns:
        flattened_list = []

        for sublist in df_column:
            for word in sublist:
                flattened_list.append(word)
                counts = Counter(flattened_list)
                counts_dict[df_column.name] = counts

    results = []

    # Print results for each column on separate lines
    for column_name, counts in counts_dict.items():
        # print(f"Word counts for {column_name}:")
        for word, count in counts.items():
            # print(f"{word}: {count}")
            results.append({'column': column_name, 'word': word, 'count': count})
        # print()

    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values(by=['column','count'], ascending=[False,False])
    results_df.to_csv('apparition_word_counts.csv', index=False)
    return counts_dict, results_df

results_apparition_extract, results_df = count_word_frequncy(df['apparition_types'], df['apparition_descriptors'], df['apparition_gender'], df['apparition_age'])
print(results_apparition_extract)
print(results_df)

#Initial Audit: {'apparition_types': Counter({'Unknown': 6029, 'ghost': 2242, 'spirit': 832, 'figure': 756, 'apparition': 709, 'presence': 428, 'shadow': 410, 'orb': 292, 'mist': 117, 'witch': 104, 'phantom': 69, 'entity': 62, 'poltergeist': 49, 'specter': 45, 'demon': 33, 'vampire': 7, 'werewolf': 5, 'warlock': 3, 'troll': 3, 'imp': 3}), 'apparition_descriptors': Counter({'presence': 428, 'ghostly': 327, 'shadowy': 105, 'unusual': 60, 'uneasy': 55, 'negative': 39, 'invisible': 30, 'restless': 29, 'misty': 28, 'anomaly': 24, 'silhouette': 23, 'mischievous': 23, 'unidentified': 20, 'hooded': 19, 'playful': 17, 'ectoplasm': 16, 'malevolent': 15, 'rider': 13, 'foreboding': 13, 'vortex': 12, 'mischief': 12, 'bizarre': 11, 'affectionately': 11, 'digital': 10, 'widely': 10, 'residing': 9, 'resembling': 9, 'hazy': 9, 'walker': 8, 'faceless': 8, 'grayish': 8, 'mild': 7, 'galloping': 7, 'swirling': 7, 'unnatural': 6, 'glide': 6, 'aimlessly': 6, 'fleeting': 6, 'desecrated': 5, 'nervous': 5, 'translucent': 5, 'blob': 5, 'smoky': 4, 'robed': 4, 'doom': 4, 'heavyset': 4, 'brunette': 4, 'creeping': 4, 'undoubtedly': 3, 'tested': 3, 'wrestler': 3, 'gloomy': 1}), 'apparition_gender': Counter({'they': 2406, 'man': 1597, 'girl': 1435, 'woman': 1176, 'them': 1126, 'boy': 671, 'lady': 413, 'person': 313, 'wife': 260, 'women': 255, 'men': 252, 'female': 187, 'mother': 181, 'husband': 171, 'guy': 144, 'daughter': 142, 'father': 133, 'son': 104, 'human': 88, 'brother': 62, 'sister': 56, 'individual': 34, 'mom': 28, 'dad': 15, 'gentlemen': 5, 'dude': 1}), 'apparition_age': Counter({'children': 626, 'child': 290, 'baby': 220, 'teenager': 166, 'elderly': 76, 'adult': 42, 'senior': 29, 'infant': 24, 'youth': 14, 'minor': 9, 'elder': 8, 'newborn': 7, 'toddler': 5, 'adolescent': 3})}
#After updating lemma_pos_tokenization: {'apparition_types': Counter({'Unknown': 5876, 'ghost': 2317, 'spirit': 848, 'figure': 776, 'apparition': 757, 'shadow': 448, 'presence': 431, 'orb': 325, 'mist': 118, 'witch': 117, 'phantom': 80, 'entity': 61, 'poltergeist': 52, 'specter': 49, 'demon': 35, 'vampire': 7, 'werewolf': 6, 'troll': 4, 'imp': 4, 'warlock': 3}), 'apparition_descriptors': Counter({'Unknown': 9635, 'presence': 431, 'ghostly': 345, 'shadowy': 108, 'unusual': 63, 'uneasy': 55, 'negative': 40, 'invisible': 31, 'restless': 30, 'misty': 29, 'silhouette': 24, 'anomaly': 24, 'mischievous': 23, 'unidentified': 21, 'ectoplasm': 20, 'playful': 17, 'malevolent': 16, 'hooded': 16, 'walker': 15, 'rider': 14, 'vortex': 13, 'mischief': 13, 'digital': 11, 'doom': 11, 'bizarre': 11, 'affectionately': 11, 'glide': 10, 'faceless': 10, 'widely': 10, 'hazy': 9, 'grayish': 8, 'mild': 7, 'unnatural': 6, 'aimlessly': 6, 'foreboding': 6, 'blob': 6, 'nervous': 5, 'translucent': 5, 'undoubtedly': 4, 'smoky': 4, 'heavyset': 4, 'brunette': 4, 'wrestler': 3, 'fleeting': 3, 'galloping': 2, 'robed': 2, 'gloomy': 1, 'swirling': 1, 'resembling': 1, 'desecrated': 1, 'residing': 1, 'creeping': 1}), 'apparition_gender': Counter({'Unknown': 4237, 'they': 2409, 'man': 1619, 'girl': 1456, 'woman': 1192, 'them': 1128, 'boy': 684, 'lady': 466, 'person': 316, 'women': 267, 'men': 265, 'wife': 262, 'female': 199, 'mother': 188, 'husband': 171, 'daughter': 145, 'guy': 145, 'father': 142, 'son': 106, 'human': 90, 'brother': 72, 'sister': 66, 'individual': 34, 'mom': 29, 'dad': 16, 'gentlemen': 5, 'dude': 1}), 'apparition_age': Counter({'Unknown': 9677, 'children': 660, 'child': 295, 'baby': 224, 'teenager': 176, 'elderly': 76, 'adult': 45, 'senior': 35, 'infant': 24, 'youth': 16, 'minor': 10, 'elder': 10, 'newborn': 8, 'toddler': 5, 'adolescent': 3})}
#After fixing misspellings: {'apparition_types': Counter({'Unknown': 5867, 'ghost': 2318, 'spirit': 858, 'figure': 776, 'apparition': 762, 'shadow': 448, 'presence': 431, 'orb': 325, 'mist': 118, 'witch': 117, 'phantom': 80, 'entity': 61, 'poltergeist': 53, 'specter': 49, 'demon': 35, 'vampire': 7, 'werewolf': 6, 'troll': 4, 'imp': 4, 'warlock': 3}), 'apparition_descriptors': Counter({'Unknown': 9635, 'presence': 431, 'ghostly': 345, 'shadowy': 108, 'unusual': 63, 'uneasy': 55, 'negative': 40, 'invisible': 31, 'restless': 30, 'misty': 29, 'silhouette': 24, 'anomaly': 24, 'mischievous': 23, 'unidentified': 21, 'ectoplasm': 20, 'playful': 17, 'malevolent': 16, 'hooded': 16, 'walker': 15, 'rider': 14, 'vortex': 13, 'mischief': 13, 'digital': 11, 'doom': 11, 'bizarre': 11, 'affectionately': 11, 'glide': 10, 'faceless': 10, 'widely': 10, 'hazy': 9, 'grayish': 8, 'mild': 7, 'unnatural': 6, 'aimlessly': 6, 'foreboding': 6, 'blob': 6, 'nervous': 5, 'translucent': 5, 'undoubtedly': 4, 'smoky': 4, 'heavyset': 4, 'brunette': 4, 'wrestler': 3, 'fleeting': 3, 'galloping': 2, 'robed': 2, 'gloomy': 1, 'swirling': 1, 'resembling': 1, 'desecrated': 1, 'residing': 1, 'creeping': 1}), 'apparition_gender': Counter({'Unknown': 4237, 'they': 2409, 'man': 1619, 'girl': 1456, 'woman': 1192, 'them': 1128, 'boy': 684, 'lady': 466, 'person': 316, 'women': 267, 'men': 265, 'wife': 262, 'female': 199, 'mother': 188, 'husband': 171, 'daughter': 145, 'guy': 145, 'father': 142, 'son': 106, 'human': 90, 'brother': 72, 'sister': 66, 'individual': 34, 'mom': 29, 'dad': 16, 'gentlemen': 5, 'dude': 1}), 'apparition_age': Counter({'Unknown': 9677, 'children': 660, 'child': 295, 'baby': 224, 'teenager': 176, 'elderly': 76, 'adult': 45, 'senior': 35, 'infant': 24, 'youth': 16, 'minor': 10, 'elder': 10, 'newborn': 8, 'toddler': 5, 'adolescent': 3})}

# Show the most common apparition types, excluding "Unknown"
for column, counts in results_apparition_extract.items():
    print(f"Most common words for {column}:")

    # Get the most common counts, excluding "Unknown"
    most_common_counts = [(word, count) for word, count in counts.most_common(10) if word.lower() != "unknown"]

    for word, count in most_common_counts:
        print(f"{word}: {count}")

    print()

#count up the number of unique apparition terms mentioned within the description
df['unique_apparition_mentions'] = df['apparition_types'].apply(len)

df_grouped_state_apparitions = df.groupby(['state'])['unique_apparition_mentions'].sum().reset_index().sort_values(by='unique_apparition_mentions', ascending=False)
df_grouped_state_apparitions

df_grouped_city_apparitions = df.groupby(['city'])['unique_apparition_mentions'].sum().reset_index().sort_values(by='unique_apparition_mentions', ascending=False)
df_grouped_city_apparitions

df_grouped_geo_apparitions = df.groupby(['location'])['unique_apparition_mentions'].sum().reset_index().sort_values(by='unique_apparition_mentions', ascending=False)
df_grouped_geo_apparitions

"""### Apparition Types (Unknown Results)"""

#Audit Recall rate of the Apparition_type feature parser
unknown_count = df['apparition_types_str'].str.lower().eq('unknown').sum()  # Count 'unknown' (case insensitive)
total_count = len(df)  # Total number of rows

print(f"Unknown count: {unknown_count}, Total row count: {total_count}, Unknown %: {round(unknown_count/total_count*100,1)}%, Classified %:{100-round(unknown_count/total_count*100,1)}%")

#Intial Audit: Unknown count: 6029, Total row count: 10992, Unknown %: 54.8%, Classified %:45.2%
#After lemma_pos_tokenized: Unknown count: 5876, Total row count: 10992, Unknown %: 53.5%, Classified %:46.5%
#After fixing misspellings: Unknown count: 5867, Total row count: 10992, Unknown %: 53.4%, Classified %:46.6%

"""### Word Embedding
A Word2Vec model is trained to see what words are closely associated to the keyword bank. The model is trained off of the haunted_dataset's descripions. This can be used to inform of apparition_type descriptors and identifiers.
Takes around 1min 30sec to train the Word2Vec model.
"""

import random
import numpy as np
#!pip install gensim
from gensim.models import Word2Vec

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

def word_embeddings(df, tokenized_corpus):
    # Train Word2Vec
    model = Word2Vec(
        df[tokenized_corpus],  # all tokenized sentences
        vector_size=100,  # dimensionality of embeddings
        window=5,         # context window size
        min_count=3,      # ignore words appearing <2 times
        workers=1,        # single-threaded (for reproducibility)
        hs=0,             # use hierarchical softmax
        negative=5,       # negative sampling
        sg=1,             # use skip-gram (instead of CBOW)
        seed=SEED,        # random seed for reproducibility
        epochs=15         # More epochs for deeper training
    )
    return model

model = word_embeddings(df,'lemma_filtered_tokenized') #Uses the lemma_filtered_tokenized so that Proper Nouns are removed

# Using the trained model, Find similar words to keyword_bank
def word2vec_associations(model, keyword_bank_df, column_name, output_csv_path):
    results = []
    for word in keyword_bank_df[column_name]:
        if isinstance(word, str):  # Check if word is a string
            try:
                similar_words = model.wv.most_similar(word, topn=15)
                for similar_word, score in similar_words:
                    results.append({"keyword": word, "similar_word": similar_word, "score": score})
                print(f"Words similar to {word}: {model.wv.most_similar(word, topn=10)}")
            except KeyError:
                print(f"{word} not found in corpus")
        else:
            continue

    df_results = pd.DataFrame(results)

    df_results.to_csv(output_csv_path, index=False)
    print(f"Results saved to {output_csv_path}")
    return df_results, model

apparition_associations, model = word2vec_associations(model, keyword_bank_df, 'Apparition_Types', 'word2vec_apparition_types.csv')
print()

"""### Apparition Descriptors"""

import pandas as pd

# Convert apparition types to a set for faster lookups
apparition_set = set(keyword_bank_df['Apparition_Types'].str.lower())

def extract_app_adjectives(tokens):
    adj_with_apparition = []  # Store adjectives with associated apparition word

    for i, (word, tag) in enumerate(tokens):
        if tag.startswith('JJ'):  # If it's an adjective
            if i > 0 and tokens[i-1][0].lower() in apparition_set: # Checks if app_kw is 1 word before adj
                adj_with_apparition.append((word, tokens[i-1][0]))
            elif i > 0 and tokens[i-2][0].lower() in apparition_set: # Checks if app_kw is 2 word before adj
                adj_with_apparition.append((word, tokens[i-2][0]))
            elif i < len(tokens) - 1 and tokens[i+1][0].lower() in apparition_set: # Checks if app_kw is 1 word after adj
                adj_with_apparition.append((word, tokens[i+1][0]))
            elif i < len(tokens) - 2 and tokens[i+2][0].lower() in apparition_set: # Checks if app_kw is 2 words after adj
                adj_with_apparition.append((word, tokens[i+2][0]))

    return adj_with_apparition

# Apply function to DataFrame
df['adjectives'] = df['pos_tokenized'].apply(extract_app_adjectives)

# Print output
print(df['adjectives'])

#Spot check of line item rows for how description is pos_tagged
print(df['pos_tokenized'][0])

#count up the number of unique apparition terms mentioned within the description
df['unique_app_descriptor_mentions'] = df['apparition_descriptors'].apply(len)

df_grouped_state_app_descriptor = df.groupby(['state'])['unique_app_descriptor_mentions'].sum().reset_index().sort_values(by='unique_app_descriptor_mentions', ascending=False)
df_grouped_state_app_descriptor

for index, row in df.iterrows():
    for adj, associated in row['adjectives']:
        print(f"Row: {index+1}, Adj: {adj}, Apparition: {associated}")

#Count the most common pairs of adjective and appartition type
from collections import Counter

# Flatten list of tuples across all rows
all_adj_app_pairs = [pair for sublist in df['adjectives'] for pair in sublist]

# Count occurrences of (adjective, apparition) pairs
pair_counts = Counter(all_adj_app_pairs)

# Display the most common pairs
print(pair_counts.most_common(20))  # Top 10 most frequent adjective-apparition pairs

import matplotlib.pyplot as plt
import seaborn as sns

# Convert tuple keys to strings
adj_app_df = pd.DataFrame(
    [(f"{adj}, {app}", count) for (adj, app), count in pair_counts.items()],
    columns=['Adjective_Apparition', 'Count']
)

# Sort and select top 10 pairs
adj_app_df = adj_app_df.sort_values(by='Count', ascending=False).head(10)

plt.figure(figsize=(12, 6))
ax = sns.barplot(
    y=adj_app_df['Adjective_Apparition'],
    x=adj_app_df['Count'],
    hue=adj_app_df['Adjective_Apparition'],  # Assign `hue` to the categorical variable
    palette="viridis",
    legend=False  # Remove unnecessary legend
)

# Add count labels on the bars
for bar in ax.patches:
    plt.text(
        bar.get_width() + 0.5,  # Offset a bit to the right
        bar.get_y() + bar.get_height() / 2,  # Centered on the bar
        f'{int(bar.get_width())}',  # Convert count to int and display
        va='center',
        fontsize=12
    )

plt.xlabel("Count")
plt.ylabel("Adjective-Apparition Pair")
plt.title("Top 10 Most Common Adjective-Apparition Pairs")
plt.show()

# Count just the adjectives (ignoring apparition type)
adjective_counts = Counter([adj for adj, app in all_adj_app_pairs])

print(adjective_counts.most_common(20))  # Top 10 most frequent adjectives

"""### Cosine Similarity Comparison
This uses the Word2Vec pre-trained vectors based on Google News dataset (about 100 billion words). This comparison was to see how closely associated the keywords were to other words in the keyword bank.
https://huggingface.co/fse/word2vec-google-news-300
"""

import numpy as np
def euclidean(vec1, vec2):
    distance = np.array(vec1) -np.array(vec2)
    squared_sum = np.sum(distance**2)
    return np.sqrt(squared_sum)

def dot_product(vec1, vec2):
    dotproduct = sum(vec1[k] * vec2[k] for k in range(len(vec1)))
    return dotproduct

def vector_norm(vector):
    norm = np.sqrt(dot_product(vector,vector))
    return norm

def cosine_similarity(vec1, vec2):
    thecosine = 0
    thedotproduct = dot_product(vec1, vec2)
    thecosine = thedotproduct/(vector_norm(vec1)*vector_norm(vec2))
    thecosine = np.round(thecosine,4)
    return thecosine

"""### Google News Word2Vec Model

This is downloading a pre-trained Word2Vec model to see if there are further word embedding associations that can highlight new words to add to the keyword bank.
This takes around 9 minutes to download.
"""

import gensim.downloader as api
vectors = api.load('word2vec-google-news-300')

# Load known apparition keywords (ensure lowercase & drop NaNs)
kws_apparition = set(keyword_bank_df['Apparition_Types'].dropna().str.lower())
kws_app_descriptors = set(keyword_bank_df['Apparition_Descriptors'].dropna().str.lower())

def similarity_comparison(keywords_1, keywords_2):
    for word in keywords_1:
        closest_word = None
        min_dist = float('inf')

        for compare_word in keywords_2:
            if word == compare_word:
                continue

            distance = euclidean(vectors[word], vectors[compare_word])

            if distance < min_dist:
                min_dist = distance
                closest_word = compare_word

        cosine_sim_calc = cosine_similarity(vectors[word], vectors[closest_word])
        print(f"Closest word to '{word}' is '{closest_word}' with min_dist of {min_dist:.3f} and cosine_similarity of {cosine_sim_calc}")
similarity_comparison(kws_apparition,kws_apparition)
print()
similarity_comparison(kws_apparition,kws_app_descriptors)

def find_closest_words(keyword, all_unique_words):
    if keyword not in vectors:
        print(f"Keyword '{keyword}' not found in Word2Vec model.")
        return []

    closest_words = []
    keyword_vector = vectors[keyword]

    for word in all_unique_words:
        if word in vectors and word != keyword:
          distance = euclidean(keyword_vector, vectors[word])
          closest_words.append((word, distance, cosine_similarity(keyword_vector, vectors[word])))

    # Sort by distance (closest first)
    closest_words.sort(key=lambda x: x[1]) #Sorts by 2nd element of the tuple (distance)
    return closest_words[:10]  # Get the top 10 closest words

for keyword in kws_apparition:
    closest_words = find_closest_words(keyword, kws_apparition)
    print(f"\nClosest words to '{keyword}':")
    for word, distance, cosine_sim in closest_words:
      print(f" - '{word}' with distance {distance:.3f} and cosince similarity {cosine_sim:.4f}")

for keyword in kws_app_descriptors:
    closest_words = find_closest_words(keyword, kws_app_descriptors)
    print(f"\nClosest words to '{keyword}':")
    for word, distance, cosine_sim in closest_words:
      print(f" - '{word}' with distance {distance:.3f} and cosince similarity {cosine_sim:.4f}")

"""## Haunted Places Dates

The code block below to get the dates will take around an 1hr to run. This block can be skipped if there are time constraints and the following code block that reads in the "haunted_places_with_dates.csv" which is a pre-loaded dataset with the expected output of the "Haunted Places Date" feature.
"""

#Need to install datefinder
!pip install datefinder
!pip install wikipedia

# This approach uses a mixed pretrained model and conventional package

import pandas as pd
import re
import datefinder
import wikipedia
from dateutil import parser
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import requests
import time
import pickle
from bs4 import BeautifulSoup
import lxml  # Ensure lxml is explicitly imported

CACHE_FILE = "wikipedia_cache.pkl"

# Load Wikipedia cache if it exists
try:
    with open(CACHE_FILE, "rb") as f:
        wikipedia_cache = pickle.load(f)
except FileNotFoundError:
    wikipedia_cache = {}

def save_cache():
    with open(CACHE_FILE, "wb") as f:
        pickle.dump(wikipedia_cache, f)

def haunted_places_datetime_df(df):
    """Processes a DataFrame of haunted places and extracts relevant dates."""
    current_year = datetime.now().year

    date_patterns = [
        r"\b(19\d{2}|20\d{2})\b",
        r"\b(\d{1,2})/(\d{1,2})/(19\d{2}|20\d{2})\b",
        r"\b(\d{1,2})-(\d{1,2})-(19\d{2}|20\d{2})\b",
        r"\b(19\d{2}|20\d{2})/\d{2}\b",
        r"\b(\w{3,9}) (19\d{2}|20\d{2})\b",
        r"\b(19\d{2}|20\d{2})-(\d{2})-(\d{2})\b",
        r"\b(\w{3,9}) (\d{1,2}), (19\d{2}|20\d{2})\b",
        r"\b(\d{1,2}) (\w{3,9}) (19\d{2}|20\d{2})\b",
        r"\b(\d{1,2}) (\w{3,9}), (19\d{2}|20\d{2})\b",
        r"\b(\w{3,9}) (\d{2}) (19\d{2}|20\d{2})\b",
        r"\b(\d{1,2})(th|st|nd|rd)? (\w{3,9}) (19\d{2}|20\d{2})\b",
        r"(\d+) years ago",
        r"\b(early|mid|late) (19\d{2}|20\d{2})s\b"
    ]

    def verify_date_with_wikipedia(place_name):
        if place_name in wikipedia_cache:
            return wikipedia_cache[place_name]

        retries = 3
        for _ in range(retries):
            try:
                truncated_name = place_name[:300]  # Ensure query doesn't exceed Wikipedia's limit
                summary = wikipedia.summary(truncated_name, sentences=2)
                soup = BeautifulSoup(summary, "lxml")
                matches = list(datefinder.find_dates(soup.text))
                if matches:
                    date_found = matches[0].strftime('%Y/%m/%d')
                    wikipedia_cache[place_name] = date_found
                    save_cache()
                    return date_found if datetime.strptime(date_found, '%Y/%m/%d').year <= current_year else '2025/01/01'
            except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError, wikipedia.exceptions.WikipediaException, requests.exceptions.RequestException):
                time.sleep(2)
        return None

    def extract_date(description):
        if not isinstance(description, str):
            return '2025/01/01'

        matches = list(datefinder.find_dates(description, source=True))
        valid_dates = []
        for date, text in matches:
            if re.search(r"19\d{2}|20\d{2}", text):
                try:
                    parsed_date = parser.parse(text, fuzzy=True, ignoretz=True)
                    if parsed_date.year <= current_year:
                        valid_dates.append(parsed_date.strftime('%Y/%m/%d'))
                except ValueError:
                    continue

        if valid_dates:
            return valid_dates[0] if datetime.strptime(valid_dates[0], '%Y/%m/%d').year <= current_year else '2025/01/01'

        relative_match = re.search(r"(\d+) years ago", description)
        if relative_match:
            years_ago = int(relative_match.group(1))
            relative_year = current_year - years_ago
            if relative_year <= current_year:
                return f"{relative_year}/01/01"

        for pattern in date_patterns:
            match = re.search(pattern, description)
            if match:
                try:
                    parsed_date = parser.parse(match.group(), fuzzy=True, ignoretz=True)
                    return parsed_date.strftime('%Y/%m/%d') if parsed_date.year <= current_year else '2025/01/01'
                except ValueError:
                    continue

        place_name = description.split('.')[0]
        wiki_date = verify_date_with_wikipedia(place_name)
        if wiki_date:
            return wiki_date

        return '2025/01/01'

    with ThreadPoolExecutor(max_workers=8) as executor:
        df['Haunted Places Date'] = list(executor.map(extract_date, df['description']))

    df['Haunted Places Date'] = pd.to_datetime(df['Haunted Places Date'], errors='coerce').dt.strftime('%Y/%m/%d')
    df.loc[df['Haunted Places Date'].isna() | (pd.to_datetime(df['Haunted Places Date']).dt.year > current_year), 'Haunted Places Date'] = '2025/01/01'

    return df

df = haunted_places_datetime_df(df)
df.head()

df_dates = pd.read_csv("haunted_places_with_dates.csv", sep=",")
df_dates.head()

df = df.merge(df_dates[['description', 'Haunted Places Date']], on='description', how='left')
df.head()

"""# Merging Provided Datasets

## Alcohol Abuse Statistics Merge
"""

def merge_haunted_data_with_alcohol_abuse_df(df):
    """Merges a haunted places DataFrame with alcohol abuse statistics."""
    # Pull in formatted table from alcohol abuse statistics
    url = 'https://drugabusestatistics.org/alcohol-abuse-statistics/'
    tables = pd.read_html(url)
    # Set df_alcohol equal to formatted table from url
    df_alcohol = tables[0]
    # Keep only necessary columns
    columns_to_keep = ['State', 'Total Deaths', '% Under 21']
    df_alcohol_filtered = df_alcohol[columns_to_keep]
    # Rename state column in the haunted DataFrame to match alcohol abuse statistics
    df_haunted = df.rename(columns={'state': 'State'})
    # Merge haunted places data with alcohol abuse statistics on 'State'
    merged_df = pd.merge(df_haunted, df_alcohol_filtered, on='State', how='left')
    df = merged_df
    return df

df = merge_haunted_data_with_alcohol_abuse_df(df)
df.head()

"""## Daylight by State Merge - USNO Navy

The block below takes around 45min to run with the cache file and 6hrs without. Run the subsequent block below it.
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import pickle
import os

def parse_date(date_str):
    """Parses date strings in multiple formats and extracts the year, month, and day."""
    for fmt in ("%m/%d/%Y", "%Y/%m/%d"):
        try:
            date_obj = datetime.strptime(date_str, fmt)
            month_map = {
                'Jan': 'Jan.', 'Feb': 'Feb.', 'Mar': 'Mar.', 'Apr': 'Apr.', 'May': 'May',
                'Jun': 'June', 'Jul': 'July', 'Aug': 'Aug.', 'Sep': 'Sep.', 'Oct': 'Oct.', 'Nov': 'Nov.', 'Dec': 'Dec.'
            }
            return date_obj.year, month_map.get(date_obj.strftime('%b'), date_obj.strftime('%b')), date_obj.day
        except ValueError:
            continue
    return None, None, None

def load_cache(cache_file):
    """Loads cached daylight data from a pickle file."""
    if os.path.exists(cache_file):
        with open(cache_file, 'rb') as f:
            return pickle.load(f)
    return {}

def save_cache(cache, cache_file):
    """Saves daylight data cache to a pickle file."""
    with open(cache_file, 'wb') as f:
        pickle.dump(cache, f)

def extract_daylight_data(response_text, month, day):
    """Parses the HTML response and extracts the daylight duration for the given month and day."""
    soup = BeautifulSoup(response_text, 'html.parser')

    tables = soup.find_all('table')
    if not tables:
        return 'No data found (Check Response Format)'

    table = None
    for tbl in tables:
        if "Duration of Daylight" in tbl.text:
            table = tbl
            break

    if not table:
        return 'No data found (Check Table Structure)'

    rows = table.find_all('tr')
    if len(rows) < 3:
        return 'No data found (Check Table Structure)'

    headers = [cell.text.strip() for cell in rows[1].find_all('td')]
    if not headers:
        return 'No data found (Check Header Structure)'

    if month not in headers:
        return 'No data found (Month Not Available)'

    month_index = headers.index(month)

    for row in rows[2:]:  # Skip header rows
        columns = row.find_all('td')
        if columns and columns[0].text.strip().isdigit():
            row_day = int(columns[0].text.strip())
            if row_day == day:
                return columns[month_index].text.strip() if month_index < len(columns) else 'No data found'

    return 'No data found (Day Not Available)'

def get_daylight_duration(year, month, day, latitude, longitude, cache, cache_file):
    """Fetches daylight duration data from the US Naval Observatory with caching."""
    cache_key = (year, month, day, latitude, longitude)
    if cache_key in cache:
        return cache[cache_key]

    url = f'https://aa.usno.navy.mil/calculated/durdaydark?year={year}&task=0&lat={latitude:.4f}&lon={longitude:.4f}&label=&submit=Get+Data'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    for attempt in range(3):  # Retry up to 3 times
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # Raises an error for HTTP issues

            daylight_data = extract_daylight_data(response.text, month, day)
            cache[cache_key] = daylight_data
            save_cache(cache, cache_file)
            return daylight_data

        except requests.exceptions.RequestException as e:
            time.sleep(2)  # Wait before retrying

    return 'Request Error'

def process_haunted_places_df(df, cache_file="daylight_cache.pkl"):
    """Processes the haunted places DataFrame and fetches daylight duration."""

    df_cleaned = df[['State', 'description', 'city', 'longitude', 'latitude', 'Haunted Places Date']].copy()

    df_cleaned[['Year', 'Month', 'Day']] = df_cleaned['Haunted Places Date'].apply(
        lambda x: pd.Series(parse_date(str(x)) if pd.notna(x) else (None, None, None))
    )

    df_cleaned = df_cleaned.dropna(subset=['longitude', 'latitude', 'Year', 'Month', 'Day'])

    df_cleaned['longitude'] = df_cleaned['longitude'].astype(float)
    df_cleaned['latitude'] = df_cleaned['latitude'].astype(float)

    # Load existing cache
    cache = load_cache(cache_file)

    df_cleaned['Daylight Data USNO Navy'] = df_cleaned.apply(
        lambda row: get_daylight_duration(row['Year'], row['Month'], row['Day'], row['latitude'], row['longitude'], cache, cache_file),
        axis=1
    )

    # Drop unnecessary columns
    df_cleaned = df_cleaned.drop(columns=['Year', 'Month', 'Day'], errors='ignore')
    df = df_cleaned

    return df

df = process_haunted_places_df(df)
df.head()

df_daylight = pd.read_csv("finalized_df_text_adding_timeanddate.csv", sep=",")
df_daylight.head()

df = df.merge(df_daylight[['description', 'Daylight Data USNO Navy']], on='description', how='left')
df.head()

df_city_sort = df.sort_values(by='city')
df_city_sort.head()

"""## Daylight by State Merge - TimeandDate"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pickle
import os

def parse_date(date_str):
    """Parses date strings in multiple formats and extracts the year, month, and day."""
    for fmt in ("%m/%d/%Y", "%Y/%m/%d"):
        try:
            date_obj = datetime.strptime(date_str, fmt)
            return date_obj.year, date_obj.month, date_obj.day
        except ValueError:
            continue
    return None, None, None

def load_cache(cache_file):
    """Loads cached daylight data from a pickle file."""
    if os.path.exists(cache_file):
        with open(cache_file, 'rb') as f:
            return pickle.load(f)
    return {}

def save_cache(cache, cache_file):
    """Saves daylight data cache to a pickle file."""
    with open(cache_file, 'wb') as f:
        pickle.dump(cache, f)

def format_city_state(city, state):
    """Formats the city and state for the Time and Date website URL using the state abbreviation."""
    city = city.lower().replace(" ", "-").replace(".", "").replace(",", "")
    state_abbreviation = state.lower()[:2]  # Use only the first two letters as the state abbreviation
    return f"{city}-{state_abbreviation}", city

def get_daylight_from_timeanddate(city, state, month, day, year, cache, cache_file):
    """Fetches daylight duration from Time and Date website with caching and error handling."""
    cache_key = (city, state, month, day, year)
    if cache_key in cache:
        return cache[cache_key]

    formatted_location, city_only = format_city_state(city, state)
    url_variants = [
        f"https://www.timeanddate.com/sun/usa/{formatted_location}?month={month}&year={year}",
        f"https://www.timeanddate.com/sun/usa/{city_only}?month={month}&year={year}"
    ]

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    for url in url_variants:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            daylight_table = soup.find("table", id="as-monthsun")
            if not daylight_table:
                continue

            rows = daylight_table.find_all("tr", attrs={"data-day": str(day)})
            if not rows:
                continue

            cols = rows[0].find_all("td")
            if len(cols) > 2:
                daylight_duration = cols[2].text.strip().split(":")[:2]  # Extract only HH:MM format
                daylight_duration = daylight_duration[0] + ":" + daylight_duration[1]  # Ensure HH:MM format
                cache[cache_key] = daylight_duration
                save_cache(cache, cache_file)
                return daylight_duration
        except requests.exceptions.RequestException:
            continue

    return "No data found"

def process_haunted_places_timeanddate_df(df, cache_file="timeanddate_cache.pkl"):
    """Processes the haunted places DataFrame and fetches daylight duration from Time and Date."""

    df[['Year', 'Month', 'Day']] = df['Haunted Places Date'].apply(
        lambda x: pd.Series(parse_date(str(x)) if pd.notna(x) else (None, None, None))
    )

    df = df.dropna(subset=['Year', 'Month', 'Day'])

    cache = load_cache(cache_file)

    df['Daylight Data TimeandDate'] = df.apply(
        lambda row: get_daylight_from_timeanddate(row['city'], row['State'], row['Month'], row['Day'], row['Year'], cache, cache_file),
        axis=1
    )

    df = df.drop(columns=['Year', 'Month', 'Day'])

    return df

df = process_haunted_places_timeanddate_df(df)
df.head()

df_timeanddate = pd.read_csv("finalized_df_text_adding_timeanddate.csv", sep=",")
df_timeanddate.head()

df = df.merge(df_timeanddate[['description','Daylight Data TimeandDate']], on='description', how='left')
df.head()

"""# 3 MIME Type Datasets

## FBI Crime Data
https://realtimecrimeindex.com/
* FBI defined geographical regions (Midwest/Northwest/South/West)
* Crime types (broad and sub-types) with count and 12 month rolling sum
* date (Month and Year), no days

In 2021, 37% of police departments nationwide, including Los Angeles and New York, stopped reporting crime data to the FBI.
https://crimeresearch.org/2024/04/how-reliable-are-the-fbis-report-of-violent-crime-data-there-are-some-major-problems/
https://www.themarshallproject.org/2023/07/13/fbi-crime-rates-data-gap-nibrs
"""

fbi_df = pd.read_csv('FBI_crime_data.csv')
print(fbi_df.describe())

state_row_count = fbi_df.groupby('State').size()
print(state_row_count)

filtered_fbi_df = fbi_df.loc[(fbi_df['Agency'] == 'Full Sample') & (fbi_df['State'] != 'Nationwide') & (fbi_df['Source.Type'] == 'Aggregate')]

print(filtered_fbi_df.head())

agg_fbi_df = filtered_fbi_df.groupby('State').sum(numeric_only=True).reset_index()
print(agg_fbi_df.describe())
print(agg_fbi_df.columns)

for column_name in ['Murder', 'Rape', 'Robbery', 'Aggravated Assault', 'Burglary', 'Theft', 'Motor Vehicle Theft', 'Violent Crime', 'Property Crime']:
    agg_fbi_df[f'{column_name} per capita'] = agg_fbi_df[column_name]/agg_fbi_df['FBI.Population.Covered']

print(agg_fbi_df.columns)

sorted_VCPC_fbi_df = agg_fbi_df.sort_values(by=['Violent Crime per capita'], ascending=False)
print(sorted_VCPC_fbi_df[['State', 'Violent Crime per capita']].head(10))

sorted_PCPC_fbi_df = agg_fbi_df.sort_values(by=['Property Crime per capita'], ascending=False)
print(sorted_PCPC_fbi_df[['State', 'Property Crime per capita']].head(10))

print(agg_fbi_df.columns)
print(agg_fbi_df.head())

"""### FBI Join with Haunted Df (Aggregated)"""

print(df.head())

#Grouping haunted df by states and returning sums
agg_df_by_state = df.groupby('state_abbrev').sum(numeric_only=True).reset_index()

#Merging aggregated datasets
merged_fbi_df = pd.merge(agg_df_by_state, agg_fbi_df, left_on='state_abbrev', right_on = 'State', how = 'left')
print(merged_fbi_df.head())
merged_fbi_df.to_csv('merged_fbi_df.csv')

df_state_row_counts = df.groupby('state_abbrev').size().reset_index(name='haunted_entry_counts')
print(df_state_row_counts.head())

merged_fbi_df = merged_fbi_df.merge(df_state_row_counts, on='state_abbrev', how='left')
print(merged_fbi_df[['state_abbrev', 'haunted_entry_counts']].head())

merged_fbi_df['apparition_per_state_entry'] = merged_fbi_df['unique_apparition_mentions']/merged_fbi_df['haunted_entry_counts']
print(merged_fbi_df[['state_abbrev', 'haunted_entry_counts', 'apparition_per_state_entry']])

merged_fbi_df_sorted = merged_fbi_df.sort_values(by=['unique_apparition_mentions', 'Violent Crime'], ascending=[False, False])

# Display the top 5 rows after sorting
print(merged_fbi_df_sorted[['state_abbrev', 'unique_apparition_mentions', 'Violent Crime', 'Property Crime','Violent Crime per capita', 'Property Crime per capita']].head(10))

import numpy as np
# Replace zeros in Property Crime and Violent Crime columns with NaN
merged_fbi_df_sorted['Violent Crime'] = merged_fbi_df_sorted['Violent Crime'].replace(0, np.nan)
merged_fbi_df_sorted['Property Crime'] = merged_fbi_df_sorted['Property Crime'].replace(0, np.nan)

# Calculate the ratio between 'unique_apparition_mentions' and 'Violent Crime'
merged_fbi_df_sorted['Apparition_ViolentCrime_Ratio'] = merged_fbi_df_sorted['unique_apparition_mentions'] / merged_fbi_df_sorted['Violent Crime']

# Calculate the ratio between 'unique_apparition_mentions' and 'Property Crime'
merged_fbi_df_sorted['Apparition_PropertyCrime_Ratio'] = merged_fbi_df_sorted['unique_apparition_mentions'] / merged_fbi_df_sorted['Property Crime']

# Display the first few rows to check the new columns
print(merged_fbi_df_sorted[['state_abbrev', 'unique_apparition_mentions', 'Violent Crime', 'Property Crime', 'Apparition_ViolentCrime_Ratio', 'Apparition_PropertyCrime_Ratio']].head())

# Group by state_abbrev and calculate the mean of the ratios
# Selecting only the numeric columns for the mean calculation
grouped_by_apparition_violentcrime = merged_fbi_df_sorted.groupby('state_abbrev')[['Apparition_ViolentCrime_Ratio']].mean()
grouped_by_apparition_propertycrime = merged_fbi_df_sorted.groupby('state_abbrev')[['Apparition_PropertyCrime_Ratio']].mean()

# Sort the ratios in descending order
sorted_by_apparition_violentcrime = grouped_by_apparition_violentcrime.sort_values(by='Apparition_ViolentCrime_Ratio', ascending=False)
sorted_by_apparition_propertycrime = grouped_by_apparition_propertycrime.sort_values(by='Apparition_PropertyCrime_Ratio', ascending=False)

# Print the top 5 states sorted by Apparition-to-Violent Crime Ratio
print("Top 10 States by Apparition-to-Violent Crime Ratio:")
print(sorted_by_apparition_violentcrime.head(10))

# Print the top 5 states sorted by Apparition-to-Property Crime Ratio
print("\nTop 10 States by Apparition-to-Property Crime Ratio:")
print(sorted_by_apparition_propertycrime.head(10))

"""### FBI Join with Haunted Df (Line item)"""

merged_fbi_df = pd.merge(agg_df_by_state, agg_fbi_df, left_on='state_abbrev', right_on = 'State', how = 'left')

select_cols_fbi_df = merged_fbi_df[['state_abbrev', 'FBI.Population.Covered', 'Murder per capita', 'Violent Crime per capita', 'Property Crime per capita']]

# Merge the aggregated ratios and calculations back into the original DataFrame to prepare it for Tika Similarity
df_merged_with_fbi = pd.merge(df, select_cols_fbi_df, on='state_abbrev', how='left')
df_merged_with_fbi.to_csv('fbi_merged_line_item.csv', index=False)

df = df_merged_with_fbi

print(df.columns)

"""## US Leading Causes of Death
https://catalog.data.gov/dataset/nchs-leading-causes-of-death-united-states

This dataset presents the age-adjusted death rates for the 10 leading causes of death in the United States beginning in 1999.

Data are based on information from all resident death certificates filed in the 50 states and the District of Columbia using demographic and medical characteristics. Age-adjusted death rates (per 100,000 population) are based on the 2000 U.S. standard population. Populations used for computing death rates after 2010 are postcensal estimates based on the 2010 census, estimated as of July 1, 2010. Rates for census years are based on populations enumerated in the corresponding censuses. Rates for non-census years before 2010 are revised using updated intercensal population estimates and may differ from rates previously published.

**Note: The code below uses the age adjusted death rate rather than the # of deaths**
"""

import numpy as np
import pandas as pd
import json
import requests
import warnings
warnings.filterwarnings(
    action='ignore', category=UserWarning, message=r"Boolean Series.*"
)

# get column names from JSON dictionary
def get_json_column_names(data: dict) -> list[str]:
    column_names = list()

    columns_frame = data['meta']['view']['columns']
    for frame in columns_frame:
        column_name = frame['name']
        column_names.append(column_name)

    return column_names

# process JSON into dictionary
def json_to_df(json_path: str) -> pd.DataFrame:
    # read json into dict using json.load()
    with open(json_path, 'r') as json_file:
        data = json.load(json_file)

    # get columns
    column_names = get_json_column_names(data=data)

    # get rows
    df_json = pd.DataFrame(columns=column_names)
    for index, datapoint in enumerate(data['data']):
        df_json.loc[index] = datapoint

    selected_columns = ['State', 'Cause Name', 'Year', 'Deaths', 'Age-adjusted Death Rate']
    return df_json[selected_columns]

# aggregrate JSON dictionary into JSON dataframe (df_final)
def aggregate_df_json(df_json: pd.DataFrame) -> pd.DataFrame:
    # AGGREGRATION: aggregate mean with tuple (state, cause name) as index
    df = df_json.groupby(["State", "Cause Name"]).agg('mean').drop('Year', axis=1)

    columns = ["State", "Cause Name", "Deaths", "Age-adjusted Death Rate"]
    df_agg = pd.DataFrame(columns=columns)

    # unpack index
    for index, row in df.iterrows():
        state, cause = index
        deaths = row['Deaths']
        age_death = row['Age-adjusted Death Rate']
        df_agg.loc[len(df_agg)] = [state, cause, deaths, age_death]

    # ORGANIZE COLUMNS: [state, death_rate{cause[0]}, death_rate{cause[1]}, ...]
    # Yields df_final
    states = list(set(df_agg.State.values))
    causes = list(set(df_agg["Cause Name"].values))
    columns = ["state"] + [f"death_rate_{cause}" for cause in causes]

    df_final = pd.DataFrame(columns=columns)
    for state in states:
        row = [state]
        for cause in causes:
            death_rate = df_agg[df_agg.State == state][df_agg['Cause Name'] == cause]["Age-adjusted Death Rate"].values[0]
            row.append(death_rate)
        df_final.loc[len(df_final)] = row

    return df_final

# join Haunted DataFrame (df) with JSON DataFrame (json_path -> df_json -> df_agg)
def join(df: pd.DataFrame, json_path: str) -> pd.DataFrame:
    #df_json = json_to_df(json_path=json_path) # process json into df, might take a couple minutes
    #df_json.to_csv('df_json.csv', index=False) # can also be found in google drive
    df_json = pd.read_csv('df_json.csv')

    # aggregate on state
    df_agg = aggregate_df_json(df_json=df_json).set_index('state')
    #print(df_agg)

    # join manually, df.join() not working
    new_col_names = list(df_agg.columns)[1:]
    new_col_dict = {}
    for new_col in new_col_names:
        new_col_dict[new_col] = list()

    for index, row in df.iterrows():
        state = row['State']
        for col in new_col_names: # death_rate_{cause}
            if state == 'Washington DC':
                new_col_dict[col].append(None)
                continue
            value = df_agg.loc[state][col]
            new_col_dict[col].append(value)

    for key, value in new_col_dict.items():
        df[key] = value

    return df

df = df.copy()
json_path = 'causes.json' # can be found in google drive
df_joined = join(df=df, json_path=json_path)

print(f"Columns: {list(df_joined.columns)}")
print("")
df_joined.to_csv('deathCauses_merged_line_item.csv', index=False)
print("Joined dataset saved in 'deathCauses_merged_line_item.csv'")

# Set new df columns from 'deathCauses_merge_line_item.csv'
df_joined = pd.read_csv('deathCauses_merged_line_item.csv')
for column in [x for x in list(df_joined.columns) if x.startswith('death_rate_')]:
    df[column] = df_joined[column].astype(float)

"""Analysis of Death Data:"""

#lists each state, the amount of hauntings, and the death rates

import pandas as pd
from google.colab.data_table import DataTable  # Enables interactive display in Colab

df_copy = df.copy()

df_copy.columns = df_copy.columns.str.lower().str.strip()

#Count the number of haunted encounters per state
haunted_counts_by_state = df_copy["state"].value_counts().rename("haunted_encounter_count")

#Select death rate columns
column_subset = df_copy.iloc[:, 10:22]  # Adjust this range if columns are misaligned

#Compute the mean values for each state for selected columns
state_aggregated_data = df_copy.groupby("state")[column_subset.columns].mean(numeric_only=True)

#Merge the haunted count with the death rate columns
final_df = haunted_counts_by_state.to_frame().merge(state_aggregated_data, left_index=True, right_index=True)

csv_filename = "states_death_rates.csv"
final_df.to_csv(csv_filename, index=True)

display(DataTable(final_df))

df.columns

import matplotlib.pyplot as plt
import seaborn as sns
import os
import numpy as np

# Create a copy of df to avoid modifying original data
df_copy = df.copy()

# Normalize column names
df_copy.columns = df_copy.columns.str.lower().str.strip()

# Count the number of haunted encounters per state
haunted_counts_by_state = df_copy["state"].value_counts().rename("haunted_encounter_count")

death_cause_columns = [col for col in df_copy.columns if "death_rate" in col.lower()]

# Compute the mean values for each state for death rate columns
state_aggregated_data = df_copy.groupby("state")[death_cause_columns].mean(numeric_only=True)

# Merge haunted counts with death rate columns
final_df = haunted_counts_by_state.to_frame().merge(state_aggregated_data, left_index=True, right_index=True)

# Save the processed dataset
csv_filename = "states_death_rates.csv"
final_df.to_csv(csv_filename, index=True)
print(f"Data saved to: {csv_filename}")


display(DataTable(final_df))

correlations = {}
for col in death_cause_columns:
    correlation = np.corrcoef(final_df[col], final_df["haunted_encounter_count"])[0, 1]
    correlations[col] = correlation

print("\nCorrelation values between hauntings and death rates:")
for col, value in correlations.items():
    print(f"{col}: {value:.2f}")

output_folder = "death_rates_scatter_plots"
os.makedirs(output_folder, exist_ok=True)

for col in death_cause_columns:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=final_df[col], y=final_df["haunted_encounter_count"])
    plt.title(f"Hauntings vs {col} (Corr: {correlations[col]:.2f})")
    plt.xlabel(col)
    plt.ylabel("Number of Haunted Encounters")

    plot_filename = f"{output_folder}/scatter_{col}.png"
    plt.savefig(plot_filename)
    print(f"Saved: {plot_filename}")

    plt.show()

"""MERGE all 3 MIME dataset back into main "df"

# JPEG

Original Idea using computer vision library to read jpeg files and obtain results of education of undergraduate graduation rates, high school graduation rates, and STEM graduation rates. although lots of fine tune testing was done, results are subpar, so will yeiled very poor results. This block only displays for the undergrad.jpeg file
"""

# THIS CODE yields very poor results :(
import cv2
!pip install pytesseract
import pytesseract
import re
import matplotlib.pyplot as plt

image_path = "undergrad.jpeg"
image = cv2.imread(image_path)
if image is None:
    raise ValueError(f"Image not found at {image_path}")

# original image shape
h, w, _ = image.shape
print("Original image dimensions:", image.shape)

# crop image
cropped = image[50:h-50, 0:w]


# grayscale image for readability
gray = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)

# apply adaptive thresholding to handle variations in text/background
thresh = cv2.adaptiveThreshold(gray, 255,
                               cv2.ADAPTIVE_THRESH_MEAN_C,
                               cv2.THRESH_BINARY, 11, 2)

# display the preprocessed image
plt.imshow(thresh, cmap='gray')
plt.title("Preprocessed Image")
plt.axis("off")
plt.show()

# running OCR
custom_config = r'--oem 3 --psm 6'
extracted_text = pytesseract.image_to_string(thresh, config=custom_config)
print("OCR extracted text:\n", extracted_text)

# try and extract abbrivs and data
pattern = r"([A-Z]{2})\s*([0-9]{1,3}\.?[0-9]*%)"
matches = re.findall(pattern, extracted_text)
print("Extracted state data:", matches)

# post-process the results to remove all invalid values
valid_states = {"AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL",
                "GA", "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA",
                "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", "NE",
                "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK",
                "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT",
                "VA", "WA", "WV", "WI", "WY", "DC"}
extracted_data = [(state, perc) for state, perc in matches if state in valid_states]
print("Filtered extracted data:", extracted_data)

"""Solution? unfortunatly the only solution at this time was to record manually from each image and append them to df as the new columns."""

import pandas as pd
# grad rates undergrad
graduation_rates_college = {
    "AK": 26.4,
    "AL": 51.8,
    "AR": 46.9,
    "AZ": 26.3,
    "CA": 64.1,
    "CO": 54.1,
    "CT": 68.0,
    "DE": 65.3,
    "FL": 56.1,
    "GA": 42.6,
    "HI": 53.3,
    "ID": 47.4,
    "IL": 61.5,
    "IN": 61.1,
    "IA": 68.5,
    "KS": 53.4,
    "KY": 49.2,
    "LA": 49.8,
    "ME": 58.1,
    "MD": 68.3,
    "MA": 73.5,
    "MI": 62.0,
    "MN": 64.9,
    "MS": 51.9,
    "MO": 56.5,
    "MT": 47.3,
    "NE": 60.7,
    "NV": 45.1,
    "NH": 67.7,
    "NJ": 66.3,
    "NM": 43.6,
    "NY": 66.0,
    "NC": 61.9,
    "ND": 52.1,
    "OH": 58.4,
    "OK": 46.3,
    "OR": 62.4,
    "PA": 67.0,
    "RI": 71.1,
    "SC": 61.9,
    "SD": 51.5,
    "TN": 50.7,
    "TX": 54.9,
    "UT": 54.5,
    "VT": 67.1,
    "VA": 67.0,
    "WA": 69.3,
    "WV": 47.0,
    "WI": 63.2,
    "WY": 58.2
}
# high school grad
graduation_rates_HS = {
    "AK": 80.4,
    "AL": 91.7,
    "AR": 87.6,
    "AZ": 77.8,
    "CA": 84.5,
    "CO": 81.1,
    "CT": 88.5,
    "DE": 89.0,
    "FL": 87.2,
    "GA": 82.0,
    "HI": 85.2,
    "ID": 80.8,
    "IL": 86.2,
    "IN": 87.2,
    "IA": 91.6,
    "KS": 87.2,
    "KY": 90.6,
    "LA": 80.1,
    "ME": 87.4,
    "MD": 86.9,
    "MA": 88.0,
    "MI": 84.1,
    "MN": 83.7,
    "MS": 85.0,
    "MO": 89.7,
    "MT": 86.6,
    "NE": 88.4,
    "NV": 84.1,
    "NH": 88.4,
    "NJ": 90.6,
    "NM": 75.1,
    "NY": 82.8,
    "NC": 86.5,
    "ND": 88.3,
    "OH": 82.0,
    "OK": 84.9,
    "OR": 80.0,
    "PA": 86.5,
    "RI": 83.9,
    "SC": 81.1,
    "SD": 84.1,
    "TN": 90.5,
    "TX": 90.0,
    "UT": 87.4,
    "VT": 84.5,
    "VA": 87.5,
    "WA": 81.1,
    "WV": 91.3,
    "WI": 90.1,
    "WY": 82.1
}
# stem grad rate
graduation_rates_stem_percent = {
    "AK": 23,
    "AL": 21,
    "AR": 22,
    "AZ": 13,
    "CA": 23,
    "CO": 25,
    "CT": 22,
    "DE": 21,
    "FL": 19,
    "GA": 22,
    "HI": 17,
    "ID": 22,
    "IL": 19,
    "IN": 22,
    "IA": 20,
    "KS": 19,
    "KY": 18,
    "LA": 19,
    "ME": 20,
    "MD": 30,
    "MA": 22,
    "MI": 22,
    "MN": 16,
    "MS": 19,
    "MO": 19,
    "MT": 25,
    "NE": 18,
    "NV": 16,
    "NH": 21,
    "NJ": 25,
    "NM": 23,
    "NY": 19,
    "NC": 23,
    "ND": 22,
    "OH": 21,
    "OK": 21,
    "OR": 19,
    "PA": 21,
    "RI": 18,
    "SC": 21,
    "SD": 27,
    "TN": 16,
    "TX": 25,
    "UT": 26,
    "VT": 17,
    "VA": 22,
    "WA": 23,
    "WV": 18,
    "WI": 21,
    "WY": 34
}

# helper to format with % sign
def format_percent(value):
    return f"{value:.1f}%" if pd.notnull(value) else None

# map dict values to columns and append
df["Undergrad_Grad_Rate"] = df["state_abbrev"].map(graduation_rates_college).apply(format_percent)
df["HS_Grad_Rate"] = df["state_abbrev"].map(graduation_rates_HS).apply(format_percent)
df["STEM_Grad_Percentage"] = df["state_abbrev"].map(graduation_rates_stem_percent).apply(format_percent)

# testing
# print(df.head())

# save
# df.to_csv("df_with_img_feat.csv", index=False)

df.head()

"""#Merge all df

"""

#With assumption that all code above is downloaded, this is the point where we can download the dataframe into 1 CSV with all the added features.

df.to_csv('finalized_df.csv', index=False)
print(df.columns)

"""# Tika Similarity and Cluster Analysis

GitHub link for source code: https://github.com/Dark-eXe/haunted-places-similarity
"""